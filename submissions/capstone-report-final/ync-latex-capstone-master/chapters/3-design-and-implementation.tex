% Chapter Template
\chapter{Design and Implementation} % Main chapter title


\label{design-and-implementation} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\subsection{Backus-Naur Form (BNF)}

Before writing a parser for a language we must understand the specification of the language. Coq consists of various sublanguages for different purposes, each with their own specifications available in the documentation. The language used to write Coq proofs is the Gallina specification language, which interacts with the language of tactics. These sublanguages are specified in Backus-Naur Form.

The Backus-Naur Form is a notation describing a context-free grammar. A BNF specification consists of expressions on the right - consisting of both 'terminals' (literals) and 'non-terminals' (variables) - represented by non-terminals on the left.

BNF specifications for different languages will have particular conventions but follow the same overall structure.  For example, here is a fragment of the BNF specification for the vernacular of Gallina:

\begin{minted}[fontsize=\scriptsize]{text}
    assertion          ::=  assertion_keyword ident [binders] : term .
    assertion_keyword  ::=  Theorem | Lemma
                            Remark | Fact
                            Corollary | Property | Proposition
                            Definition | Example
    proof              ::=  Proof . ... Qed .
                            Proof . ... Defined .
                            Proof . ... Admitted .
\end{minted}
\subsection{The grammar module: a BNF-inspired data structure}
\emph{Code referenced here from \path{jeremy-parser/grammar.py}.}

\code{proof-reader} parses a given input string into a syntax tree which can be easily traversed and thereby evaluated. In parsing a language, there are many levels of abstraction that we can utilize. The highest level of abstraction would be a parser generator that directly accepts a BNF specification and returns a fully functioning parser with zero or minimal lines of code needed (I explain why this path is not taken in \ref{using-parser-generator} \nameref{using-parser-generator}.) Through trial and error we eventually arrived at a mid-level abstraction. We define a data structure that resembles BNF and dictates the flow of the parser, provided by the \code{grammar} module.

Instead of hard-coding conditional branches to determine matching patterns at each step, the \code{grammar} module provides the \code{GRAMMAR} variable, which is \textbf{a map (Python dictionary) from a rule to a tuple containing a regular expression and a list of valid child rules}. As the parser constructs a syntactical unit and its children, it performs lookups to the data structure to understand what rule patterns to apply at each step.

Here is a truncated version of the actual \code{GRAMMAR} map, with ellipses representing truncated rules; the full object is in the \code{grammar.py} file. As you can see, each rule mirrors a non-terminal on the left, and its regular expression and children  represent its expression on the right. Note the indentation does not correspond to actual nesting of data, but is intended to visually reflect the nested definitions.

\begin{minted}[fontsize=\scriptsize]{python}
import * from constants
GRAMMAR = {
    LABEL_DOCUMENT: 
        (None,
        [LABEL_PROOF,
         LABEL_REQUIRE_IMPORT,
         LABEL_SEARCH, #...]),
        #...
        LABEL_PROOF: 
            (fr"{KW_PROOF}\.(.*?)(?:{KW_QED}|{KW_ADMITTED}|{KW_ABORT})\.", 
            [LABEL_APPLY,
             LABEL_ASSERT, #...]),
            #...
            LABEL_REWRITE:
                (fr"{KW_REWRITE}({REGEXP_REWRITE_ARROW}?{REGEXP_TERM_OPTIONAL_SPACE})\
                {REGEXP_IN_OCCURRENCE}{REGEXP_AT_OCCURRENCE}{REGEXP_TACTIC_END}\
                {REGEXP_TACTIC_LOOKAHEAD}",
                [LABEL_REWRITE_ARROW,
                 LABEL_TERM]),

                LABEL_REWRITE_ARROW: 
                    (fr"({REGEXP_REWRITE_ARROW})", []),
                #...
    #...
}
\end{minted}

For example, the \code{PROOF} rule states: \emph{"a proof is a lazily matched substring beginning with the keyword 'Proof' plus a period, and ends with either the keyword 'Qed', 'Admitted', or 'Abort', plus a period. The inner string must consist of any number of child components matching the rules \code{APPLY}, \code{ASSERT}, etc"}.

Observe:
\begin{itemize}
    \item For readability and modularity, we factor out keywords and regular expression fragments into the \code{constants.py} module. We interpolate these constants into regex patterns via Python raw f-strings.
    \item A terminal node has an empty rule list. For example, \code{LABEL\_REWRITE\_ARROW} is a terminal.
    \item This data structure is simple. While it serves the purpose of this project, see \ref{limitations-grammar} \nameref{limitations-grammar}.
\end{itemize}


Having generated a syntax tree, we are now in a position to traverse and evaluate it.

\subsection{Parsing input to generate a syntax tree}
\emph{Code referenced here from \path{jeremy-parser/grammar.py}.}

\subsubsection{Constructing syntax tree nodes}
\label{constructing-nodes}
First, we define a \code{Node} object.
\begin{minted}{python}
class Node:
def __init__(self, label: str, val: str = None, children: list = None):
    # Each node's label identifies its "type", i.e. which syntactical unit it represents.
    self.label = label
    # Each node's value is the contents of the substring it matched on. This is useful for logging and constructing helpful warning messages.
    self.val = val
    # Each node has a list of children, since each syntactical unit may be comprised of sub-components. Hence a node with no children is a terminal node.
    self.children = children or []
\end{minted}

The main parser function \code{construct\_node} is responsible for recursively constructing a syntax tree composed of \code{Node} objects, and returning it. It accepts an input string \code{s} and \code{rule}, the label of the rule that the string has been matched on.

\begin{minted}[fontsize=\scriptsize]{python}
def construct_node(s: str, rule: str) -> Node:
    def construct_children(s: str, parent: str, acc: list) -> List[Node]:
        if not s:
            return None, acc, ""
        _, expected = grammar.GRAMMAR[parent]
        if expected == []:
            return s, acc, ""
        exception = None
        for item in expected:
            pattern, _ = grammar.GRAMMAR[item]
            match = re.match(pattern, s)
            if not match:
                continue
            if item == LABEL_TERM:
                term_s, remaining_s = get_next_subterm(match.group(1))
                term = construct_term(term_s)
                return term_s, acc+[term], remaining_s
            try:
                child, remaining_s = construct_node_helper(
                    match.group(1), item)
                remaining_s = remaining_s + s[match.end():]
                return construct_children(remaining_s, parent, acc + [child])
            except (UnmatchedToken, UnmatchedTactic) as e:
                exception = e
        if exception:
            raise exception
        if parent == LABEL_PROOF:
            raise UnmatchedTactic(s)
        raise UnmatchedToken(s)

    def construct_node_helper(s: str, rule:str) -> (Node, str):
        term_s, children, remaining_s = construct_children(s, rule, [])
        node = Node(rule, term_s or s)
        node.children = children
        return node, remaining_s

    node, _ = construct_node_helper(s, rule)
    return node
\end{minted}

It is only called once by the main program, with the entire script as input. It initiates the construction of the tree by calling \code{construct\_node\_helper} and assumes there will be no remaining string to parse (since the root node \code{DOCUMENT} accepts the entire script).

\code{construct\_node\_helper} is responsible for recursively constructing the current subtree, which comprises of both the current node as well as all its children. It accepts an input substring \code{s} and \code{rule}, the label of the rule that the substring has been matched on. Only after successfully constructing all its children will it then create the current node and assign the children. It returns both a syntax subtree as well as the remaining string that contains sibling subtrees to be further parsed.

\code{construct\_children} accepts an input substring \code{s}, its parent \code{rule} , and an accumulator \code{acc} containing the child nodes it will return. It returns \code{term\_s} (the value of the parent substring, should this be different from the original substring \code{s}), a list of child nodes \code{children}, and the remaining string that contains sibling subtrees to be further parsed, \code{remaining\_s}. It proceeds as such:
\begin{itemize}
    \item \code{construct\_children} first performs a lookup in the grammar module to obtain the expected children of \code{rule}.
    \item For each rule, it performs a lookup in the grammar module to obtain the corresponding regular expression. It attempts to match the expression against the beginning of \code{s}.
    \item On a match, it calls \code{construct\_node\_helper} to construct the current child's subtree, and then recurses on the remaining string to construct the rest of the children.
\end{itemize}

Observe that in \code{construct\_children}, instead of recursing on the rest of the substring following the matched capture group \code{ s[match.end():]}, we return \code{remaining\_s} from \code{construct\_node\_helper}, which always demands it from \code{construct\_children}, which is ultimately returned as an empty string in \code{construct\_children}'s base cases, or extracted by \code{construct\_term}. We then recurse on \code{remaining\_s + s[match.end():]}. This is because we cannot rely solely on regular expressions to correctly capture the exact substring containing the next subtree.

When parsing a tactic invocation containing a term, we need to count the parentheses in the term to determine its endpoint. Afterwards, we can continue parsing the rest of the substring, prefixed with any extraneous characters the regular expression might have captured. Therefore, if the current substring matches a term, we make a call to \code{construct\_term}.

\subsubsection{Constructing terms and subterms}
In order to validate the arity of terms supplied to tactics such as \code{rewrite}, \code{exact} and \code{apply}, we need to parse a term into its subterms, which are grouped in nested parenthesis. Regular expressions are not expressive enough to capture nested patterns. Here is an example substring:
\begin{minted}{Coq}
exact (my_lemma_1 (my_lemma_2 n1) n2).
exact (my_lemma_3 n3).
\end{minted}
Suppose we have constructed the first \code{exact} node, and now we need to capture the parent term \code{(my\_lemma\_1 (my\_lemma\_2 n1) n2)}, before trying to capture its children \code{my\_lemma\_1}, \code{(my\_lemma\_2 n1)} and \code{n2}.
\begin{itemize}
    \item A lazy pattern on opening and closing parenthesis, such as \texttt{\textbackslash(.+?\textbackslash)}, would capture:
          \begin{minted}{Coq}
(my_lemma_1 (my_lemma_2 n1). 
          \end{minted}
    \item On the other hand, a greedy pattern like \texttt{\textbackslash(.+\textbackslash)} would capture everything until the last parenthesis in the substring:
          \begin{minted}{Coq}
(my_lemma_1 (my_lemma_2 n1) n2).exact (my_lemma_3 n3).
          \end{minted}
\end{itemize}
Hence the regular expressions used in the generic \code{construct\_node} function cannot construct this subtree, and we need a specialized \code{construct\_term} function which is able to count parentheses:

\begin{minted}{python}
def construct_term(term: str) -> Node:
    def construct_subterms(s: str) -> List[Node]:
        if s == "":
            return []
        subterm, remaining = get_next_subterm(s)
        child = construct_term(subterm)
        children = [child] + construct_subterms(remaining)
        return children
    if term and term[0] == "(" and term[-1] == ")":
        term = term[1:-1]
    node = Node(LABEL_TERM, term)
    if re.fullmatch(r"[^\s]+", term):
        return node
    node.children = construct_subterms(term)
    return node
\end{minted}
The familiar problem of counting parenthesis is implemented iterative-style in \code{get\_next\_subterm}:

\begin{minted}{python}
def get_next_subterm(s: str) -> (str, str):
    k = 0
    for i, c in enumerate(s):
        if c == " " and k == 0:
            return s[:i], s[i+1:]
        elif c == '(':
            k += 1
        elif c == ')':
            k -= 1
    if k != 0:
        raise Exception("Invalid parentheses.")
return s, ""
\end{minted}

\subsubsection{Example syntax tree}
Here is an example proof from the lecture notes of week 2 of FPP AY 19/20 Semester 1.
\begin{minted}{Coq}
Require Import Arith Bool.
Check Nat.add_0_r.
Proposition first_formal_proof :
    forall n : nat,
    n + 0 = 0 + n.
Proof.
    intro n.
    Check (Nat.add_0_r n).
    rewrite -> (Nat.add_0_r n).
    Check (Nat.add_0_l n).
    rewrite -> (Nat.add_0_l n).
    reflexivity.
Qed.
\end{minted}
Here is the resulting syntax tree, pretty-printed:
\begin{minted}[fontsize=\scriptsize]{text}
|- DOCUMENT:
"Require Import Arith Bool...Qed."
    |- REQUIRE_IMPORT:
        "Require Import Arith Bool."
    |- CHECK:
        "Check Nat.add_0_r."
    |- ASSERTION:
        "Proposition first_formal_proof : forall n : nat, n + 0 = 0 + n"
            |- ASSERTION_KEYWORD:
            "Proposition"
            |- ASSERTION_IDENT:
            "first_formal_proof"
            |- FORALL:
            "n : nat"
                |- BINDER:
                    "n"
                |- TYPE:
                    "nat"
            |- ASSERTION_TERM:
            "n + 0 = 0 + n"
    |- PROOF:
        "intro n.Check (Nat.add_0_r n).rewrite -> (Nat.add_0_r n).Check (Nat.add_0_l n).rewrite -> (Nat.add_0_l n).reflexivity."
            |- INTRO:
            "n"
            |- CHECK:
            "Check (Nat.add_0_r n)."
            |- REWRITE:
            "(Nat.add_0_r n)"
                |- REWRITE_ARROW:
                    " -> "
                |- TERM:
                    "Nat.add_0_r n"
                        |- TERM:
                        "Nat.add_0_r"
                        |- TERM:
                        "n"
            |- CHECK:
            "Check (Nat.add_0_l n)."
            |- REWRITE:
            "(Nat.add_0_l n)"
                |- REWRITE_ARROW:
                    " -> "
                |- TERM:
                    "Nat.add_0_l n"
                        |- TERM:
                        "Nat.add_0_l"
                        |- TERM:
                        "n"
            |- REFLEXIVITY:
            "reflexivity."
\end{minted}
Observe:
\begin{itemize}
    \item \code{ASSERTION} subtrees have \code{ASSERTION\_IDENT} as well as \code{BINDER} arguments so the \code{collect\_arity} function simply has to store the identifier with the number of arguments.
    \item \code{TERM} subtrees have subterms as children, so the \code{check\_arity} function simply has to validate the number of terms at each depth.
\end{itemize}

\subsection{Feature 1: Recognizing unpermitted tactics}
In general, when \code{proof-reader} encounters unsupported syntax, it will not continue parsing since it is impossible to differentiate the unsupported syntactical unit and the rest of the script. Thus it raises the \code{UnmatchedToken} exception (see \code{construct\_node} in \ref{constructing-nodes} \nameref{constructing-nodes}), which will be used to display the remaining substring.

When it encounters an unrecognized token that might be an unpermitted tactic invocation – i.e. any unsupported syntax within a \code{PROOF} node – it raises \code{UnmatchedTactic} instead, which attempts to extract a specific tactic invocation from the remaining substring. This is used to display a more precise warning message, such as the one in \ref{example-1} \nameref{example-1}.

\begin{minted}{python}
class UnmatchedToken(Exception):
    def __init__(self, remaining):
        self.remaining = remaining

class UnmatchedTactic(Exception):
    def __init__(self, remaining):
        self.remaining = remaining
        self.tactic = None
        match = re.match(fr"(.+?){REGEXP_TACTIC_END}
        {REGEXP_TACTIC_LOOKAHEAD}",
                         self.remaining)
        if match:
            self.tactic = match.group(1)
\end{minted}

\subsection{Feature 2: Verifying arity}
\emph{Code referenced here from \path{jeremy-parser/proof_reader.py}.}
Having constructed a syntax tree without errors, we are now in a position to traverse and evaluate it. As we traverse the syntax tree, we need to compare the arity of its assertions with assertions that have been defined in the environment – both assertions that have been defined earlier in the script, as well as predefined theorems from built-in libraries that have been imported.

\subsubsection{Collecting arity signatures of built-in library theorems}
We must have a database of arities for built-in library theorems. This only needs to be done once, or whenever new modules are added to the course (see Appendix on repository for modules used in the course).

For each module, we manually issue a Search command, e.g. \code{Search \_ inside Nat} to list all theorem definitions in the response buffer. We save each list as a string, process the string so it resembles Coq code, and generate a syntax tree containing those definitions using the same \code{construct\_tree} used for input code. We then run \code{collect\_arity}, defined below, returning the dictionary \code{arity\_db}, a map from assertion names to arities. We save this dictionary as a file, to be loaded whenever \code{proof-reader} is run. Later, we add user-defined lemmas to this dictionary we parse the input syntax tree.

This procedure is performed by the script \path{jeremy-parser/script_parse_theory_lib.py}.

\begin{minted}{python}
def collect_arity(t: Node, arity_db : dict) -> dict:
    assert(t.label == LABEL_DOCUMENT)
    for child in t.children:
        if child.label != LABEL_ASSERTION:
            continue
        assertion = child
        if len(assertion.children) < 3: 
            continue
        ident = assertion.children[1]
        forall = assertion.children[2]
        binders = [c for c in forall.children if c.label == LABEL_BINDER]
        arity = len(binders)
        arity_db[ident.val] = arity
    return arity_db
\end{minted}

\subsubsection{Collecting arity signatures of the syntax tree}
In \code{check\_arity}, before evaluating the input syntax tree, we use the same \code{collect\_arity} function to collect arity signatures of assertions that have been defined in the script.

\begin{minted}{python}
def check_arity(t: Node, arity_db: dict) -> list:
    #...
    def check_subterms(subterms: list, parent_term: Node, parent_tactic_label: str):
        #...
    def traverse(t: Node):
        #...
    arity_db = collect_arity(t, arity_db)
    traverse(t)
    return warnings
\end{minted}
We could collect arity signatures and check arity at the same time instead of two separate steps, therefore performing a single traversal instead of two traversals, since the tree traversal is left-to-right, and assertions must be declared before they are referenced. This was the approach in an earlier implementation. However, we implement two separate traversals for the sake of readability. Since each traversal is done in constant time, there is insignificant difference in actual performance. Furthermore, Coq does not allow repeat definitions so we will not have to deal with multiple signatures for the same assertion.

\subsubsection{Checking arity of the syntax tree}
The \code{check\_arity} function can be found in\code{jeremy-parser/proof\_reader.py}. It is responsible for traversing the input syntax tree \code{t} given a map of arity signatures \code{arity\_db}, and returning a list of warnings indicating instances where the expected arity is not equal to the actual arity. Expected arity is the integer value found in \code{arity\_db} if the assertion has been defined. Actual arity is the number of subterms following the first term. \code{check\_arity} simply explores relevant nodes –\code{REWRITE}, \code{EXACT}, and \code{APPLY} – and counts the number of subterms at each level, generating a warning when arity is incorrect.

\subsection{User interface}
\emph{See \path{jeremy-parser/proof-reader.el} in the repository.}

We implement a user interface by defining a simple Emacs command. The command calls the Proof General function \code{proof-process-buffer} to process the current buffer and verify that the script is syntactically correct. It uses the hook \code{proof-shell-handle-error-or-interrupt-hook} to terminate if Proof General emits an error, and only runs the Python program when there is no error. Providing an Emacs command allows students to seamlessly check their script without leaving the editor. The book "Writing GNU Emacs Extensions" \parencite{glickstein} was helpful.

\subsection{Extending \code{proof-reader}}
\label{extending}
The grammar module abstraction enables syntax to be extended conveniently. To extend the supported syntax - for example, adding a permitted tactic - the instructor simply has to add a rule definition to the grammar module, comprising of a regular expression and the expected child rules. Not all desired syntax may be expressible in the current framework; see \ref{limitations-grammar} \nameref{limitations-grammar}.

\subsection{Unit and acceptance testing}
\emph{Code referenced here from \path{jeremy-parser/tests.py}.}

\code{TestParser} performs unit tests to verify that \code{construct\_node} fulfils its specifications. For each test input it compares the generated syntax tree with the expected syntax tree. Each unit test verifies that variations of a particular syntactical unit is correctly parsed.

\code{TestParserAcceptance} also performs acceptance tests for \code{construct\_node}, running it on sample student submissions which have provided by the instructor.

\code{TestParityCheck} performs unit tests to verify that \code{collect\_arity} and \code{arity\_check}. It generates and evaluates the syntax tree for each test input, and compares the output warnings with the expected warnings. It performs both positive tests (inputs that should trigger no warnings) and negative tests (inputs that should trigger warnings), and each input contains variations of \code{exact}, \code{rewrite} and \code{apply} syntax.