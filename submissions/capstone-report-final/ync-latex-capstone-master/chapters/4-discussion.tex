% Chapter Template

\chapter{Discussion} % Main chapter title

\label{dicussion} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}


\section{Time and space complexity}
Note that the generated syntax trees are expected to be shallow with some constant depth, with most branches having only a few levels, since the grammar contains non-recursive definitions. The only source of arbitrary depth is \code{TERM}, but the expected depth of \code{TERM} subtrees for actual student submissions is around 1-3 levels, since highly nested terms are quite rare in simple proofs, and might call for a goal to be factored into a standalone lemma.

\subsection{Regular expression matching in \code{construct\_node}}
Regular expressions can be computationally expensive - for example, they might grow exponentially in complexity when catastrophic backtracking occurs. However, all the regex patterns in the \code{grammar} module use lazy quantifiers, thus avoiding backtracking. Furthermore, almost all matches are performed on Coq sentences or fragments of sentences, which are very short substrings. Lastly, since we only accept input that is syntactically validated by \code{coqtop}, the input is quite predictable.

Therefore we can reasonably assume total $O(N)$ time where $ N $ is the length of the input string; the tree is expected to be shallow, so the same substring will only be processed some constant number of times. We also have $ O(1) $ space complexity. Counting parentheses in \code{get\_next\_subterm} incurs $ O(N) $ time complexity as well.

\subsection{Constructing and traversing the syntax tree}
The \code{construct\_node} function takes $ O(N) $ time to construct the entire tree, and $ O(N) $ space on the callstack (since \code{construct\_node} is recursive and Python does not have tail-call optimization), where $ N $ is the length of the input string and the total number of nodes constructed is proportional to $ N $. A more precise estimate might be $O(log_kN)$ where $k$ is the average number of nodes generated for each level of the tree. Due to the space consumption, Python's recursion limit should be increased if long proofs are expected (currently set to 10000).

\code{collect\_arity} and \code{arity\_check} will explore nodes only at the first and second level, incurring a $ O(N) $ time complexity and  $ O(1) $ space complexity.

\subsection{Overall complexity}
Therefore, the overall complexity for constructing and evaluating the syntax tree is $ O(N) $ time and $ O(N) $ space. We also run performance tests on real input as well as large input, the results of which can be found in the Appendix on the repository.

\section{Limitations of the \code{grammar} module}
\label{limitations-grammar}
The grammar module is limited in its expressiveness because of its simple structure. Firstly, rules do not directly express patterns with distinct subcomponents. For example, an assertion is broken down into a keyword, identifier, 'for all' statement, and a term:

\begin{minted}{python}
LABEL_ASSERTION: 
    (fr"({REGEXP_ASSERTION} .+?:.+?)\.{REGEXP_DOC_LOOKAHEAD}",
    [LABEL_ASSERTION_KEYWORD,
     LABEL_ASSERTION_IDENT,
     LABEL_FORALL,
     LABEL_ASSERTION_TERM])
\end{minted}

However, since \code{construct\_node} only expects one capture group, it must iteratively match all subcomponent rules on that single captured substring after the substring has been matched on the assertion. The pattern cannot express subcomponents that appear in different locations of the substring. Furthermore, the structure does not express which subcomponents are required. Each subcomponent rule is treated as optional; it could match on only one of them once, or one of them repeatedly, and as long as the string is eventually consumed the assertion subtree will be treated as a successful match. One solution is for the parser to iterate over a list of capture groups and a list of expected patterns, or a map of named capture groups and their expected patterns.

Secondly, rules do not express alternate patterns. A single regex pattern defines the acceptable structure for each rule. So far we are able to express alternate patterns within the regular expressions as a single pattern with alternating parts. However, a better solution is to define a rule as a list of pattern/children pairs instead of a single pair, with each pair representing an additional matching option.

Fortunately, these issues do not seem to pose a problem for the input we have tested on, but making the module more expressive would make matching more precise, and improve extensibility.

\section{Alternative approaches}

\subsection{Modifying Coqtop source code}
Given that Coq's ability to infer missing arguments is an additional feature, it seems natural to modify the source code to provide an option to turn the feature off, as opposed to building an external parser from scratch. There are two reasons why this approach was not taken:

Firstly, and most importantly, a source code approach reduces usability and maintainability of the tool. Unless my changes are accepted into the master branch of the open-source Coq codebase on Github, students will be locked in to the Coq version I worked with. They would not benefit from updates to Coq and might have limited access to the Coq ecosystem. The features described in this report are quite prescriptive and highly specific to FPP's learning goals. While the tool has broader applications as an approach to educators with similar goals, its features may not be relevant to the average Coq user. Granted, we could maintain a modified branch in a separate repository, into which we merge Coq updates. But this involves repeated reintegration and might also introduce dependency or installation issues.

Secondly, a source code approach did not seem feasible. I judged that it was out of scope for this project due to my limited experience with large-scale Ocaml applications. Even if I managed to achieve my desired functionality, refactoring source code might introduce invisible bugs in other components.


\subsection{Using a parser generator}
\label{using-parser-generator}
Writing parsers for programming language is a well-understood problem, and parser generators automate the implementation of parsing algorithms. A parser generator accepts a grammar specification and produces a parser that can evaluate the specified language. I spent a significant amount of time exploring the use of parser generators to build my tool. Using a parser generator was appealing because I did not want to 're-invent the wheel', and it promised quick development, high-level abstraction, and high performance.

I tried using several parser generators, for example CEDET's built-in Wisent, and the python package Lark. Each had their own issues. For example, I had some success specifying the grammar of my Coq sublanguage with Lark, but there were often bugs that I had to find workarounds for because I did not understand the error messages. The algorithms were quite complex and I did not have full visibility or understanding of the underlying processes. Furthermore, there were certain functionalities (e.g. collecting and storing previous arity signatures) that did not seem possible in the existing frameworks - modifying the source code was possible but complicated.

Ultimately, for the purposes of my project, writing a parser from scratch gave me granular control of my development process and allowed me to make informed decisions on the level of abstraction to use for different components.


\section{Future work}
Firstly, implementing a more expressive grammar structure would improve extensibility and precision, as detailed in \ref{limitations-grammar} \nameref{limitations-grammar}.

Secondly, implementing the entire program in Emacs-lisp would allow the program to be run directly in Emacs instead of as a child process. This would likely improve performance and eliminate any installation or interoperability issues.

Thirdly, modifications or extensions to the grammar should be made possible at runtime instead of requiring source-code modification. This would avoid rebuilding the project and improve the user experience for the instructor. In this implementation, this can be done by importing the grammar module from outside the binary package, similar to how a parser generator might take a specification as input. Closer integration with Proof General would also allow new library modules to be added via an Emacs command.

Fourthly, performance profiling can be run to identify bottlenecks - string slicing should be eliminated to avoid unnecessary copying, and an iterative implementation might be faster.

\section{Reflections}
In the process of developing \code{proof-reader} I gained some insights on development in general. First and foremost, we need to have clear specifications from which we can write unit tests. In this project I had a set of example inputs and expected warnings, but would have saved time if I expanded on them by running more acceptance tests earlier in order to discover more edge cases.

Secondly, we should always take advantage of any features that ensure type correctness, as early as possible (I used type annotations from Python's built-in \code{typing} module). They help us think more precisely especially in the planning phase, which is really where most of the work happens.

Lastly, in designing a program, we need to find the right level of abstraction. Even though using a parser generator was appealing, like most 'automagical' tools, it provided a high level of abstraction and fast development, at the cost of flexibility and understanding. I learned this the hard way after spending an entire semester on it only to switch approaches â€“ but it was a lesson worth learning.